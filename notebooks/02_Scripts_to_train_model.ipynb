{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save a script to train a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to test Azure MLOps tools with a classification problem. \n",
    "\n",
    "My usual test case is to classify daily particle size distributions (that was what I did in Uno when doing my PhD, so know and like the problem). In short, there are days when something happens (event days), days when something does not happen (non-event days) and mixed days (undefined days). There we were try to build a model to predict days into these three. \n",
    "\n",
    "There exists have concentration data and classification data. Both have mixed quality, some data missing and some wrong. The classification data is also provided with higher granularity than described above: there is class 1a, 1b, and 2 events. Here we however aggregate them all. \n",
    "\n",
    "This notebook creates a training script that uses Tensorflow with Keras frontend to build up a CNN classifier. The classifier is simple and bad, but it is fast to run. :) \n",
    "\n",
    "Data from training process and test data confusion matrix are saved in Azure Machine Learning workspace to keep track on the performance of the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Create a folder experiment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = '../classification_scripts'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "#print(experiment_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/aerosol_training.py\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# parameters\n",
    "\n",
    "# how to scale logaritmic data\n",
    "data_scaling={\"use_limits\":True,\n",
    "             'min':-3, \n",
    "             'max':5}\n",
    "log_offset=1e-3\n",
    "classes=3\n",
    "\n",
    "\n",
    "###\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "run.log('Info1','script started ok')\n",
    "\n",
    "###\n",
    "#load the diabetes data (passed as an input dataset)\n",
    "print(\"Loading Data...\")\n",
    "\n",
    "hyde = run.input_datasets['hyde_data'].to_pandas_dataframe()\n",
    "hyde_class=run.input_datasets['class_data'].to_pandas_dataframe()\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "hyde.sample(10).to_csv(\"outputs/sample_hydedata.csv\", index=False, header=True)\n",
    "hyde_class.sample(10).to_csv(\"outputs/sample_classdata.csv\", index=False, header=True)\n",
    "\n",
    "### Data cleaning\n",
    "# Get date in datetimeindex format, drop original columns\n",
    "\n",
    "hyde['date']=pd.to_datetime(hyde[['Year', 'Month', 'Day', 'Hour','Minute','Second']])\n",
    "hyde=hyde.drop(['Year', 'Month', 'Day', 'Hour','Minute','Second'], axis=1)\n",
    "hyde.set_index('date', inplace=True)\n",
    "\n",
    "hyde_class['date']=pd.to_datetime(hyde_class['date'])\n",
    "hyde_class.set_index('date', inplace=True)\n",
    "\n",
    "# convert columnnames to shorter, they show the particle size in nm\n",
    "dps=[(item,item[10:]) for item in hyde.columns if item.startswith('HYY_DMPS.d')]\n",
    "dps_2={name:np.round((float(item[:3])/1000* 10**(int(item[-1]))),2) for (name, item) in dps}\n",
    "cols_dict=dict(zip(hyde.columns, dps_2))\n",
    "hyde=hyde.rename(columns=dps_2)\n",
    "\n",
    "# drop metadata columns off\n",
    "metacolumns=[ 'HYY_DMPS.t1',    'HYY_DMPS.t2',    'HYY_DMPS.p1',    'HYY_DMPS.p2',\n",
    "         'HYY_DMPS.rh1',   'HYY_DMPS.rh2', 'HYY_DMPS.tconc',]\n",
    "hydemeta=hyde[metacolumns]\n",
    "hyde=hyde.drop(metacolumns, axis=1)\n",
    "\n",
    "run.log_list('hyde data columns', list(hyde.columns))\n",
    "\n",
    "###\n",
    "\n",
    "# data cleaning: replace negative values with zeros, \n",
    "# replace nan's using a linear interpolation (this is a time serie) \n",
    "\n",
    "hyde[hyde<0]=0\n",
    "hyde=hyde.fillna( method ='ffill', axis=0)\n",
    "hyde=hyde.fillna(0)\n",
    "\n",
    "###\n",
    "\n",
    "# Merge classification data and the measurement data so that they have same days\n",
    "\n",
    "data_days=hyde.groupby([hyde.index.date])\n",
    "data_days=pd.to_datetime(list(data_days.groups))\n",
    "\n",
    "class_days=hyde_class.index\n",
    "\n",
    "common_days=[item.date() for item in sorted(list(set(data_days) & set(class_days)))]\n",
    "\n",
    "\n",
    "# create hyde_crop dataset so that all days have a classification\n",
    "hyde_crop=hyde.reset_index()\n",
    "hyde_crop=hyde_crop[hyde_crop['date'].dt.date.isin(common_days)]\n",
    "hyde_crop=hyde_crop.set_index('date')\n",
    "\n",
    "\n",
    "# crop classification data so all days have aerosol data\n",
    "class_crop=hyde_class.loc[common_days]\n",
    "\n",
    "\n",
    "# test if indeces are ok\n",
    "assert set(hyde_crop.index.date)^set(class_crop.index)==set()\n",
    "\n",
    "###\n",
    "\n",
    "# Log-transform and scale the measurement data to better suit for the ML algorithm\n",
    "# this could be changed to use scalers from scikit learn\n",
    "# but now it is in line with the traditional scaling\n",
    "\n",
    "log_offset=1e-6\n",
    "hyde_log=(hyde_crop+log_offset).apply(np.log10)\n",
    "\n",
    "mini, maxi=hyde_log.min().min() , hyde_log.max().max()\n",
    "\n",
    "if data_scaling['use_limits']:\n",
    "    hyde_norm=(hyde_log-0.5*(maxi-10+mini))/(maxi-2-mini)\n",
    "else:\n",
    "    hyde_norm=(hyde_log-(data_scaling['min']+data_scaling['max'])/2)/(data_scaling['max']+data_scaling['min'])\n",
    "\n",
    "###\n",
    "\n",
    "# Create day-based arrays that are in right format for Keras model\n",
    "# data_s if not normed data\n",
    "# data_n is normed data\n",
    "\n",
    "# create an array of arrays for each day containing particle data\n",
    "#data_s = np.array([  grp.values.T for key, grp in hyde_crop.groupby([hyde_crop.index.date]) if len(grp.values)==144] )\n",
    "data_n = np.array([  grp.values.T for key, grp in hyde_norm.groupby([hyde_norm.index.date]) if len(grp.values)==144] )\n",
    "# array of dates\n",
    "dates_s=np.array([  key for key, grp in hyde_crop.groupby([hyde_crop.index.date]) if len(grp.values)==144] )\n",
    "\n",
    "#print('The size of the data matrices: '+ str(data_s.shape) + ' ,' + str(data_n.shape))\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# Make arrays from the classification data for the same dates as the hyde_data is\n",
    "# Three arrays, for three different classification schemes\n",
    "\n",
    "print(hyde_class.head())\n",
    "\n",
    "# only the relevant columns\n",
    "classes_s=hyde_class[['event Ia','event Ib','event II','non event','undefined']]\n",
    "classes_5s=classes_s[classes_s.index.isin(dates_s)]\n",
    "\n",
    "# combine all event columns to one column, build up a new df\n",
    "only_events=pd.DataFrame(classes_5s[['event Ia', 'event Ib', 'event II']].sum(axis=1))\n",
    "only_events.columns=['event']\n",
    "others=classes_5s[[ 'undefined','non event']]\n",
    "classes_3s=pd.merge(only_events, others, left_index=True, right_index=True)\n",
    "\n",
    "# combine all 1 class event columns to one column, build up a new df\n",
    "only_events=pd.DataFrame(classes_5s[['event Ia', 'event Ib']].sum(axis=1))\n",
    "only_events.columns=['event I']\n",
    "others=classes_5s[[ 'event II', 'undefined','non event']]\n",
    "classes_4s=pd.merge(only_events, others, left_index=True, right_index=True)\n",
    "\n",
    "# COnvert to numpy arrays\n",
    "classes_3=np.array(classes_3s)\n",
    "classes_4=np.array(classes_4s)\n",
    "classes_5=np.array(classes_5s)\n",
    "\n",
    "#print(classes_3.shape)\n",
    "#print(data_s.shape)\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# Split to train and test sets\n",
    "X = data_n\n",
    "X = X.reshape( X.shape+(1,)  )\n",
    "\n",
    "if classes==3:\n",
    "    y=classes_3\n",
    "elif classes==4:\n",
    "    y=classes_4\n",
    "elif classes==5:\n",
    "    y=classes_5\n",
    "    \n",
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "#print(classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=30)\n",
    "\n",
    "\n",
    "###\n",
    "# build CNN model\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=( 61,144,1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train, epochs=10,\n",
    "                    callbacks=[callback],\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "t2=time.time()\n",
    "\n",
    "run.log('model train time (secs)',round (t2 - t1))\n",
    "\n",
    "print((\"It takes %s seconds to train the model.\") % round (t2 - t1))\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "if classes==3:\n",
    "    class_names=['Event', 'Undefined','Non-event']\n",
    "elif classes==4:\n",
    "    class_names=['Event I','Event II', 'Undefined','Non-event']\n",
    "elif classes==5:\n",
    "    class_names=['Event Ia', 'Event Ib', 'Event II', 'Non-event', 'Undef']\n",
    "\n",
    "# yt: real classification for the test data\n",
    "# pr: predicted classification for the test data\n",
    "yt=y_test.argmax(axis=1)\n",
    "pr=np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "print(yt[:5])\n",
    "print(pr[:5])\n",
    "print(len(pr), len(yt))\n",
    "\n",
    "# Compute confusion matrix and tranform it to right format for log_table function\n",
    "cnf_matrix = confusion_matrix(yt, pr)\n",
    "\n",
    "cf_dict={}\n",
    "for row in range(cnf_matrix.shape[0]):\n",
    "    cf_dict[row]=list(cnf_matrix[row])\n",
    "    \n",
    "run.log_table('confusion_matrix', cf_dict)\n",
    "\n",
    "def plot_confusion_matrix(f, ax, cm, class_names,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    #This function prints and plots the confusion matrix.\n",
    "    #Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(class_names)\n",
    "\n",
    "    ax.set_xlim([-0.5,len(class_names)-0.5])\n",
    "    ax.set_ylim([-0.5,len(class_names)-0.5])\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, np.round(cm[i, j],2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    f.tight_layout()\n",
    "\n",
    "    \n",
    "    return f\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "f, ax=plt.subplots()\n",
    "plot_confusion_matrix(f, ax, cnf_matrix, class_names=class_names,\\\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "run.log_image('confusion matrix',plot=f)\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "\n",
    "model.save('outputs/aerosol_classification_model.h5')\n",
    "#joblib.dump(value=model, filename='outputs/aerosol_classification_model.pkl')\n",
    "\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
